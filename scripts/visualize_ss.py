#!/usr/bin/env python3
"""
Visualize ss (socket statistics) sampler CSV log files.

This script reads ss_stats.csv files generated by run_ss_sampler_conf*.sh and creates
visualizations for various TCP metrics:
- cwnd (congestion window size)
- rtt (round-trip time)
- ssthresh (slow start threshold)
- bytes_sent / bytes_acked (throughput estimation)
- retrans (retransmission count)
- delivery_rate / pacing_rate

Usage:
    python3 visualize_ss.py --ss-log ./exp_logs_I/ss_stats.csv --output cwnd.png
    python3 visualize_ss.py --ss-log ./exp_logs_I/ss_stats.csv --metric rtt --output rtt.png
    python3 visualize_ss.py --ss-log ./exp_logs_I/ss_stats.csv --metric all --output all_metrics.png
"""

import argparse
import csv
import os
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
import sys

try:
    # Use non-interactive backend for headless servers
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    import numpy as np
except ImportError:
    print("Error: matplotlib and numpy are required.")
    print("Install with: pip install matplotlib numpy")
    sys.exit(1)


# Supported metrics and their configurations
METRICS_CONFIG = {
    'cwnd': {
        'field': 'cwnd',
        'label': 'Congestion Window [segments]',
        'title': 'Congestion Window vs Time',
        'scale': 1,
        'aggregate': False,  # Per-flow metric
    },
    'ssthresh': {
        'field': 'ssthresh',
        'label': 'Slow Start Threshold [segments]',
        'title': 'Slow Start Threshold vs Time',
        'scale': 1,
        'aggregate': False,
    },
    'rtt': {
        'field': 'rtt_us',
        'label': 'RTT [ms]',
        'title': 'Round-Trip Time vs Time',
        'scale': 1000,  # Convert us to ms
        'aggregate': False,
    },
    'rttvar': {
        'field': 'rttvar_us',
        'label': 'RTT Variance [ms]',
        'title': 'RTT Variance vs Time',
        'scale': 1000,  # Convert us to ms
        'aggregate': False,
    },
    'bytes_sent': {
        'field': 'bytes_sent',
        'label': 'Bytes Sent [MB]',
        'title': 'Cumulative Bytes Sent vs Time',
        'scale': 1_000_000,  # Convert to MB
        'aggregate': True,
    },
    'bytes_acked': {
        'field': 'bytes_acked',
        'label': 'Bytes Acked [MB]',
        'title': 'Cumulative Bytes Acked vs Time',
        'scale': 1_000_000,  # Convert to MB
        'aggregate': True,
    },
    'retrans': {
        'field': 'retrans',
        'label': 'Retransmits [packets]',
        'title': 'Retransmits vs Time',
        'scale': 1,
        'aggregate': True,
    },
    'lost': {
        'field': 'lost',
        'label': 'Lost Packets',
        'title': 'Lost Packets vs Time',
        'scale': 1,
        'aggregate': True,
    },
    'delivery_rate': {
        'field': 'delivery_rate',
        'label': 'Delivery Rate [Mbps]',
        'title': 'Delivery Rate vs Time',
        'scale': 1_000_000,  # Convert to Mbps (assuming bps input)
        'aggregate': False,
    },
    'pacing_rate': {
        'field': 'pacing_rate',
        'label': 'Pacing Rate [Mbps]',
        'title': 'Pacing Rate vs Time',
        'scale': 1_000_000,  # Convert to Mbps
        'aggregate': False,
    },
    'throughput': {
        'field': 'bytes_acked',  # Will be calculated as derivative
        'label': 'Throughput [Mbps]',
        'title': 'Throughput vs Time (from bytes_acked)',
        'scale': 1,  # Special handling
        'aggregate': True,
        'derivative': True,  # Flag for special processing
    },
}

# Port ranges for identifying flow types (from gen_experiment.py)
CUBIC_PORT_MIN = 5201
CUBIC_PORT_MAX = 5225
PRAGUE_PORT_MIN = 5226
PRAGUE_PORT_MAX = 5250


def identify_flow_type(local_port: int, remote_port: int) -> str:
    """Identify flow type based on port numbers."""
    # Check local port first
    if CUBIC_PORT_MIN <= local_port <= CUBIC_PORT_MAX:
        return 'cubic'
    elif PRAGUE_PORT_MIN <= local_port <= PRAGUE_PORT_MAX:
        return 'prague'
    # Check remote port
    elif CUBIC_PORT_MIN <= remote_port <= CUBIC_PORT_MAX:
        return 'cubic'
    elif PRAGUE_PORT_MIN <= remote_port <= PRAGUE_PORT_MAX:
        return 'prague'
    else:
        return 'unknown'


def parse_ss_csv(filepath: str) -> Dict[str, Dict]:
    """
    Parse ss_stats.csv file and organize data by flow (identified by port pair).
    
    Returns:
        Dictionary mapping flow_id to flow data containing timestamps and metrics
    """
    flows = defaultdict(lambda: {
        'timestamps': [],
        'local_port': None,
        'remote_port': None,
        'flow_type': 'unknown',
        'metrics': defaultdict(list)
    })
    
    if not os.path.exists(filepath):
        print(f"Error: File not found: {filepath}")
        return {}
    
    try:
        with open(filepath, 'r') as f:
            reader = csv.DictReader(f)
            
            # Get first row to determine start time
            first_timestamp = None
            
            for row in reader:
                try:
                    timestamp_ns = int(row.get('timestamp_ns', 0))
                    
                    # Initialize first timestamp for relative time
                    if first_timestamp is None:
                        first_timestamp = timestamp_ns
                    
                    # Calculate relative time in seconds
                    relative_time = (timestamp_ns - first_timestamp) / 1e9
                    
                    # Get port info
                    local_port = int(row.get('local_port', 0) or 0)
                    remote_port = int(row.get('remote_port', 0) or 0)
                    
                    # Create flow identifier
                    flow_id = f"{local_port}_{remote_port}"
                    
                    flow = flows[flow_id]
                    flow['timestamps'].append(relative_time)
                    flow['local_port'] = local_port
                    flow['remote_port'] = remote_port
                    flow['flow_type'] = identify_flow_type(local_port, remote_port)
                    
                    # Extract all metrics
                    for metric_name, config in METRICS_CONFIG.items():
                        field = config['field']
                        value_str = row.get(field, '')
                        
                        # Parse value, handling empty strings and 'Mbps'/'bps' suffixes
                        try:
                            if value_str == '' or value_str is None:
                                value = 0
                            elif 'Mbps' in str(value_str):
                                value = float(value_str.replace('Mbps', '')) * 1_000_000
                            elif 'Kbps' in str(value_str):
                                value = float(value_str.replace('Kbps', '')) * 1_000
                            elif 'bps' in str(value_str):
                                value = float(value_str.replace('bps', ''))
                            else:
                                value = float(value_str)
                        except (ValueError, TypeError):
                            value = 0
                        
                        flow['metrics'][metric_name].append(value)
                
                except (ValueError, KeyError) as e:
                    continue  # Skip malformed rows
                    
    except Exception as e:
        print(f"Error reading CSV file: {e}")
        return {}
    
    # Summary
    print(f"Loaded {len(flows)} flows from {filepath}")
    for flow_id, flow_data in flows.items():
        print(f"  Flow {flow_id}: {len(flow_data['timestamps'])} samples, type={flow_data['flow_type']}")
    
    return dict(flows)


def downsample_data(timestamps: List[float], values: List[float], 
                    target_points: int = 1000) -> Tuple[np.ndarray, np.ndarray]:
    """Downsample data to reduce plot complexity while preserving shape."""
    if len(timestamps) <= target_points:
        return np.array(timestamps), np.array(values)
    
    # Use numpy for efficient downsampling
    timestamps = np.array(timestamps)
    values = np.array(values)
    
    # Simple decimation
    step = len(timestamps) // target_points
    indices = np.arange(0, len(timestamps), step)
    
    return timestamps[indices], values[indices]


def calculate_aggregate_by_type(flows: Dict[str, Dict], metric: str,
                                 time_resolution: float = 0.1,
                                 max_time: float = 480.0) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:
    """
    Calculate aggregate or average values for each flow type.
    
    Returns:
        Dictionary mapping flow_type to (time_bins, values)
    """
    config = METRICS_CONFIG.get(metric, {})
    scale = config.get('scale', 1)
    can_aggregate = config.get('aggregate', False)
    is_derivative = config.get('derivative', False)
    
    # Group flows by type
    flows_by_type = defaultdict(list)
    for flow_id, flow_data in flows.items():
        flows_by_type[flow_data['flow_type']].append(flow_data)
    
    result = {}
    
    for flow_type, type_flows in flows_by_type.items():
        time_bins = np.arange(0, max_time + time_resolution, time_resolution)
        
        if can_aggregate:
            aggregate = np.zeros(len(time_bins))
            counts = np.zeros(len(time_bins))
            
            for flow_data in type_flows:
                timestamps = flow_data['timestamps']
                
                if metric == 'throughput':
                    # Calculate throughput as derivative of bytes_acked
                    bytes_acked = np.array(flow_data['metrics'].get('bytes_acked', []))
                    times = np.array(timestamps)
                    
                    if len(bytes_acked) > 1:
                        # Compute derivative (bytes per second -> Mbps)
                        dt = np.diff(times)
                        dt[dt == 0] = 0.001  # Avoid division by zero
                        dbytes = np.diff(bytes_acked)
                        throughput = (dbytes / dt) * 8 / 1_000_000  # bits/s -> Mbps
                        values = np.concatenate([[0], throughput])
                    else:
                        values = np.zeros_like(bytes_acked)
                else:
                    values = np.array(flow_data['metrics'].get(metric, []))
                    values = values / scale if scale != 1 else values
                
                for t, v in zip(timestamps, values):
                    if t <= max_time:
                        bin_idx = int(t / time_resolution)
                        if bin_idx < len(aggregate):
                            aggregate[bin_idx] += v
                            counts[bin_idx] += 1
            
            result[flow_type] = (time_bins, aggregate)
        else:
            # For per-flow metrics, compute average across flows
            sum_values = np.zeros(len(time_bins))
            count_values = np.zeros(len(time_bins))
            
            for flow_data in type_flows:
                timestamps = flow_data['timestamps']
                values = np.array(flow_data['metrics'].get(metric, []))
                values = values / scale if scale != 1 else values
                
                for t, v in zip(timestamps, values):
                    if t <= max_time:
                        bin_idx = int(t / time_resolution)
                        if bin_idx < len(sum_values):
                            sum_values[bin_idx] += v
                            count_values[bin_idx] += 1
            
            # Compute average
            average = np.divide(sum_values, count_values,
                                out=np.zeros_like(sum_values),
                                where=count_values != 0)
            
            result[flow_type] = (time_bins, average)
    
    return result


def plot_metric(flows: Dict[str, Dict], output_file: str,
                metric: str = 'cwnd',
                title: str = None,
                show_individual: bool = False,
                max_time: float = 480.0,
                time_resolution: float = 0.1):
    """
    Plot the specified metric for all flows.
    
    Args:
        flows: Dictionary of flow data
        output_file: Output file path
        metric: Metric to plot
        title: Custom title (optional)
        show_individual: Whether to show individual flow lines
        max_time: Maximum time to plot
        time_resolution: Time bin resolution in seconds
    """
    if metric not in METRICS_CONFIG:
        raise ValueError(f"Unknown metric: {metric}. Supported: {list(METRICS_CONFIG.keys())}")
    
    config = METRICS_CONFIG[metric]
    ylabel = config['label']
    default_title = config['title']
    
    fig, ax = plt.subplots(figsize=(14, 6))
    
    # Color scheme
    colors = {
        'cubic': 'blue',
        'prague': 'orange',
        'bbr': 'green',
        'reno': 'purple',
        'unknown': 'gray'
    }
    
    # Calculate aggregated/averaged values by flow type
    data_by_type = calculate_aggregate_by_type(flows, metric, time_resolution, max_time)
    
    # Plot main lines
    for flow_type, (time_bins, values) in data_by_type.items():
        color = colors.get(flow_type, 'gray')
        label = f'{flow_type.capitalize()}'
        if not config.get('aggregate', False):
            label += ' (avg)'
        
        # Downsample for plotting if needed
        time_bins_ds, values_ds = downsample_data(time_bins, values, target_points=2000)
        ax.plot(time_bins_ds, values_ds, color=color, linewidth=1.5, label=label, alpha=0.9)
    
    # Optionally show individual flows
    if show_individual:
        scale = config.get('scale', 1)
        for flow_id, flow_data in flows.items():
            flow_type = flow_data['flow_type']
            color = colors.get(flow_type, 'gray')
            
            timestamps = np.array(flow_data['timestamps'])
            values = np.array(flow_data['metrics'].get(metric, []))
            values = values / scale if scale != 1 else values
            
            # Downsample
            timestamps_ds, values_ds = downsample_data(timestamps, values, target_points=500)
            ax.plot(timestamps_ds, values_ds, color=color, linewidth=0.3, alpha=0.3)
    
    # Add phase markers
    phase_info = [(0, "Phase 1"), (120, "Phase 2"), (240, "Phase 3"), (360, "Phase 4")]
    for t, label in phase_info:
        if t < max_time:
            ax.axvline(x=t, color='gray', linestyle=':', alpha=0.5)
            ylim = ax.get_ylim()
            ax.text(t + 2, ylim[1] * 0.95 if ylim[1] > 0 else 10,
                    label, fontsize=9, alpha=0.7)
    
    ax.set_xlabel('Time [s]', fontsize=12)
    ax.set_ylabel(ylabel, fontsize=12)
    ax.set_title(title or default_title, fontsize=14)
    ax.set_xlim(0, max_time)
    ax.set_ylim(bottom=0)
    ax.grid(True, alpha=0.3)
    ax.legend(loc='upper right')
    
    plt.tight_layout()
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"Saved {metric} plot to {output_file}")
    plt.close()


def plot_all_metrics(flows: Dict[str, Dict], output_file: str,
                     title: str = None, max_time: float = 480.0):
    """
    Plot all key metrics in a multi-panel figure.
    """
    # Metrics to plot in the combined view
    metrics_to_plot = ['cwnd', 'rtt', 'throughput', 'retrans']
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 10))
    axes = axes.flatten()
    
    colors = {
        'cubic': 'blue',
        'prague': 'orange',
        'bbr': 'green',
        'reno': 'purple',
        'unknown': 'gray'
    }
    
    for idx, metric in enumerate(metrics_to_plot):
        ax = axes[idx]
        config = METRICS_CONFIG[metric]
        ylabel = config['label']
        metric_title = config['title']
        
        # Calculate data
        data_by_type = calculate_aggregate_by_type(flows, metric, 
                                                    time_resolution=0.5, 
                                                    max_time=max_time)
        
        if not data_by_type:
            ax.text(0.5, 0.5, f'No data for {metric}',
                    transform=ax.transAxes, ha='center', va='center')
            continue
        
        for flow_type, (time_bins, values) in data_by_type.items():
            color = colors.get(flow_type, 'gray')
            label = f'{flow_type.capitalize()}'
            
            time_bins_ds, values_ds = downsample_data(time_bins, values, target_points=1000)
            ax.plot(time_bins_ds, values_ds, color=color, linewidth=1.2, label=label)
        
        # Add phase markers
        for t in [0, 120, 240, 360]:
            if t < max_time:
                ax.axvline(x=t, color='gray', linestyle=':', alpha=0.3)
        
        ax.set_xlabel('Time [s]', fontsize=10)
        ax.set_ylabel(ylabel, fontsize=10)
        ax.set_title(metric_title, fontsize=11)
        ax.set_xlim(0, max_time)
        ax.set_ylim(bottom=0)
        ax.grid(True, alpha=0.3)
        ax.legend(loc='upper right', fontsize=8)
    
    plt.suptitle(title or 'TCP Metrics from ss Sampler (Fine-grained)', 
                 fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"Saved all metrics plot to {output_file}")
    plt.close()


def plot_cwnd_detail(flows: Dict[str, Dict], output_file: str,
                     max_time: float = 480.0):
    """
    Create a detailed cwnd plot showing individual flows.
    """
    fig, axes = plt.subplots(2, 1, figsize=(16, 10), sharex=True)
    
    colors = {
        'cubic': plt.cm.Blues,
        'prague': plt.cm.Oranges,
    }
    
    flow_type_names = ['cubic', 'prague']
    
    for ax, flow_type in zip(axes, flow_type_names):
        colormap = colors.get(flow_type, plt.cm.Greys)
        
        # Get flows of this type
        type_flows = [(fid, fd) for fid, fd in flows.items() 
                      if fd['flow_type'] == flow_type]
        
        if not type_flows:
            ax.text(0.5, 0.5, f'No {flow_type} flows',
                    transform=ax.transAxes, ha='center', va='center')
            continue
        
        # Plot each flow with a different shade
        n_flows = len(type_flows)
        for i, (flow_id, flow_data) in enumerate(type_flows):
            timestamps = np.array(flow_data['timestamps'])
            cwnd = np.array(flow_data['metrics'].get('cwnd', []))
            
            if len(timestamps) == 0:
                continue
            
            # Downsample
            timestamps_ds, cwnd_ds = downsample_data(timestamps, cwnd, target_points=500)
            
            # Color based on flow index
            color = colormap(0.3 + 0.7 * i / max(n_flows - 1, 1))
            port = flow_data['local_port']
            ax.plot(timestamps_ds, cwnd_ds, color=color, linewidth=0.8, 
                    alpha=0.7, label=f'Port {port}' if n_flows <= 10 else None)
        
        # Add phase markers
        for t in [0, 120, 240, 360]:
            if t < max_time:
                ax.axvline(x=t, color='gray', linestyle=':', alpha=0.3)
        
        ax.set_ylabel('cwnd [segments]', fontsize=11)
        ax.set_title(f'{flow_type.capitalize()} Flows - Congestion Window', fontsize=12)
        ax.set_xlim(0, max_time)
        ax.set_ylim(bottom=0)
        ax.grid(True, alpha=0.3)
        if n_flows <= 10:
            ax.legend(loc='upper right', fontsize=7, ncol=2)
    
    axes[-1].set_xlabel('Time [s]', fontsize=11)
    
    plt.suptitle('Detailed Congestion Window Analysis', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"Saved detailed cwnd plot to {output_file}")
    plt.close()


def main():
    parser = argparse.ArgumentParser(
        description="Visualize ss (socket statistics) sampler CSV log files",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Visualize congestion window (default)
  python3 visualize_ss.py --ss-log ./exp_logs_I/ss_stats.csv --output cwnd.png
  
  # Visualize RTT
  python3 visualize_ss.py --ss-log ./exp_logs_I/ss_stats.csv --metric rtt --output rtt.png
  
  # Visualize throughput (calculated from bytes_acked)
  python3 visualize_ss.py --ss-log ./exp_logs_I/ss_stats.csv --metric throughput --output throughput.png
  
  # Visualize all metrics in one figure
  python3 visualize_ss.py --ss-log ./exp_logs_I/ss_stats.csv --metric all --output all_metrics.png
  
  # Detailed cwnd plot showing individual flows
  python3 visualize_ss.py --ss-log ./exp_logs_I/ss_stats.csv --metric cwnd_detail --output cwnd_detail.png

Available metrics:
  - cwnd: Congestion window in segments (average per flow type)
  - ssthresh: Slow start threshold (average per flow type)
  - rtt: Round-trip time in ms (average per flow type)
  - rttvar: RTT variance in ms (average per flow type)
  - bytes_sent: Cumulative bytes sent in MB (aggregated)
  - bytes_acked: Cumulative bytes acknowledged in MB (aggregated)
  - throughput: Throughput in Mbps (calculated from bytes_acked, aggregated)
  - retrans: Retransmission count (aggregated)
  - lost: Lost packet count (aggregated)
  - delivery_rate: Delivery rate in Mbps (average per flow type)
  - pacing_rate: Pacing rate in Mbps (average per flow type)
  - all: Plot all major metrics in one figure
  - cwnd_detail: Detailed cwnd plot showing individual flows
        """
    )
    
    parser.add_argument("--ss-log", required=True,
                        help="Path to ss_stats.csv log file")
    parser.add_argument("--output", default="ss_plot.png",
                        help="Output file name (default: ss_plot.png)")
    parser.add_argument("--metric", default="cwnd",
                        choices=['cwnd', 'ssthresh', 'rtt', 'rttvar', 'bytes_sent', 
                                 'bytes_acked', 'throughput', 'retrans', 'lost',
                                 'delivery_rate', 'pacing_rate', 'all', 'cwnd_detail'],
                        help="Metric to plot (default: cwnd)")
    parser.add_argument("--title", default=None,
                        help="Custom title for the plot")
    parser.add_argument("--max-time", type=float, default=480.0,
                        help="Maximum time to plot in seconds (default: 480)")
    parser.add_argument("--show-individual", action="store_true",
                        help="Show individual flow lines (can be slow for large datasets)")
    parser.add_argument("--time-resolution", type=float, default=0.1,
                        help="Time bin resolution in seconds (default: 0.1)")
    
    args = parser.parse_args()
    
    # Validate file
    if not os.path.isfile(args.ss_log):
        print(f"Error: File '{args.ss_log}' does not exist")
        sys.exit(1)
    
    # Load data
    print(f"Loading ss sampler log from: {args.ss_log}")
    flows = parse_ss_csv(args.ss_log)
    
    if not flows:
        print("Error: No valid flow data found in the CSV file")
        sys.exit(1)
    
    # Plot based on metric choice
    if args.metric == 'all':
        plot_all_metrics(flows, args.output, args.title, args.max_time)
    elif args.metric == 'cwnd_detail':
        plot_cwnd_detail(flows, args.output, args.max_time)
    else:
        plot_metric(flows, args.output, args.metric, args.title,
                    args.show_individual, args.max_time, args.time_resolution)
    
    print("Visualization complete!")


if __name__ == "__main__":
    main()